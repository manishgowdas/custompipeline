name: Deploy EKS Addons

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Target environment (dev, stage, prod)"
        required: true
        default: "dev"
        type: choice
        options:
          - dev
          - stage
          - prod
      addon:
        description: "Addon to deploy (alb, metric-server, argocd, prometheus, grafana, karpenter)"
        required: true
        default: "alb"
        type: choice
        options:
          - alb
          - metric-server
          - argocd
          - prometheus
          - grafana
          - karpenter
      vpc_id:
        description: "VPC ID (required only for ALB controller)"
        required: false
        type: string

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      id-token: write       # Required for OIDC
      contents: read

    steps:
    - name: Checkout repo
      uses: actions/checkout@v4

    # --- FIXED: Proper AWS Role ARN construction ---
    - name: Set AWS Role ARN
      run: |
        echo "AWS_ACCOUNT_ID=${{ secrets.AWS_ACCOUNT_ID }}" >> $GITHUB_ENV
        echo "ROLE_TO_ASSUME=arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsEKSRole" >> $GITHUB_ENV
        echo "AWS_REGION=ap-northeast-2" >> $GITHUB_ENV
    # --- Environment-to-cluster mapping ---
    - name: Set cluster name
      run: |
        case "${{ github.event.inputs.environment }}" in
          dev)   echo "CLUSTER_NAME=mycluster" >> $GITHUB_ENV ;;
          stage) echo "CLUSTER_NAME=mycluster" >> $GITHUB_ENV ;;
          prod)  echo "CLUSTER_NAME=mycluster" >> $GITHUB_ENV ;;
        esac
    # Debug output (safe)
    - name: Debug info
      run: |
        echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID"
        echo "ROLE_TO_ASSUME=$ROLE_TO_ASSUME"
        echo "Region=$AWS_REGION"
        echo "Cluster=$CLUSTER_NAME"
    - name: Configure AWS credentials via OIDC
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ env.ROLE_TO_ASSUME }}
        aws-region: ${{ env.AWS_REGION }}
        audience: sts.amazonaws.com

    - name: Setup kubectl
      uses: azure/setup-kubectl@v3

    - name: Setup helm
      uses: azure/setup-helm@v3

    - name: Update kubeconfig for EKS
      run: aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION


    # --- Metrics Server ---
    - name: Deploy Metrics Server
      if: ${{ github.event.inputs.addon == 'metric-server' }}
      run: |
        # Remove existing EKS-managed metrics-server resources
        kubectl delete deployment metrics-server -n kube-system --ignore-not-found=true
        kubectl delete service metrics-server -n kube-system --ignore-not-found=true
        kubectl delete rolebinding metrics-server-auth-reader -n kube-system --ignore-not-found=true
        kubectl delete clusterrolebinding metrics-server:system:auth-delegator --ignore-not-found=true
        kubectl delete clusterrole system:metrics-server --ignore-not-found=true
        kubectl delete serviceaccount metrics-server -n kube-system --ignore-not-found=true
        kubectl delete apiservice v1beta1.metrics.k8s.io --ignore-not-found=true
        
        # Install with Helm
        helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
        helm repo update
        helm upgrade --install metrics-server metrics-server/metrics-server \
          --namespace kube-system --create-namespace
    # --- AWS Load Balancer Controller ---
    - name: Ensure IAM Policy
      if: ${{ github.event.inputs.addon == 'alb' }}
      run: |
        curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
        POLICY_ARN="arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy"
        if ! aws iam get-policy --policy-arn $POLICY_ARN >/dev/null 2>&1; then
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam_policy.json
        else
          echo "Policy already exists, skipping..."
        fi
    - name: Ensure IAM Role
      if: ${{ github.event.inputs.addon == 'alb' }}
      run: |
        OIDC_PROVIDER=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION \
          --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")
        cat > trust.json <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Federated": "arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/${OIDC_PROVIDER}"
              },
              "Action": "sts:AssumeRoleWithWebIdentity",
              "Condition": {
                "StringEquals": {
                  "${OIDC_PROVIDER}:sub": "system:serviceaccount:kube-system:aws-load-balancer-controller"
                }
              }
            }
          ]
        }
        EOF
        if ! aws iam get-role --role-name AWSLoadBalancerControllerRole >/dev/null 2>&1; then
          aws iam create-role \
            --role-name AWSLoadBalancerControllerRole \
            --assume-role-policy-document file://trust.json
          aws iam attach-role-policy \
            --role-name AWSLoadBalancerControllerRole \
            --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy
        else
          echo "Role already exists, skipping..."
        fi
    - name: Ensure Service Account
      if: ${{ github.event.inputs.addon == 'alb' }}
      run: |
        kubectl get sa aws-load-balancer-controller -n kube-system >/dev/null 2>&1 || \
          kubectl create serviceaccount aws-load-balancer-controller -n kube-system
        kubectl annotate serviceaccount aws-load-balancer-controller \
          -n kube-system \
          eks.amazonaws.com/role-arn=arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSLoadBalancerControllerRole --overwrite
    - name: Validate VPC ID for ALB
      if: ${{ github.event.inputs.addon == 'alb' }}
      run: |
        if [ -z "${{ github.event.inputs.vpc_id }}" ]; then
          echo "Error: VPC ID is required for ALB controller deployment"
          exit 1
        fi
    
    - name: Deploy AWS Load Balancer Controller
      if: ${{ github.event.inputs.addon == 'alb' }}
      run: |
        helm repo add eks https://aws.github.io/eks-charts
        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          --namespace kube-system --create-namespace \
          --set clusterName=$CLUSTER_NAME \
          --set serviceAccount.create=false \
          --set serviceAccount.name=aws-load-balancer-controller \
          --set vpcId="${{ github.event.inputs.vpc_id }}"
    # --- ArgoCD ---
    - name: Deploy ArgoCD
      if: ${{ github.event.inputs.addon == 'argocd' }}
      run: |
        helm repo add argo https://argoproj.github.io/argo-helm
        helm upgrade --install argocd argo/argo-cd \
          --namespace argocd --create-namespace
    # --- Prometheus ---
    - name: Deploy Prometheus
      if: ${{ github.event.inputs.addon == 'prometheus' }}
      run: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo update
        helm upgrade --install prometheus prometheus-community/prometheus \
          --namespace monitoring --create-namespace \
          --set server.persistentVolume.enabled=true \
          --set server.persistentVolume.storageClass=gp2 \
          --set server.persistentVolume.size=10Gi
    # --- Grafana ---
    - name: Deploy Grafana
      if: ${{ github.event.inputs.addon == 'grafana' }}
      run: |
        helm repo add grafana https://grafana.github.io/helm-charts
        helm repo update
        helm upgrade --install grafana grafana/grafana \
          --namespace monitoring --create-namespace
    
    # --- Karpenter ---
    - name: Create Karpenter IAM Policy
      if: ${{ github.event.inputs.addon == 'karpenter' }}
      run: |
        POLICY_ARN="arn:aws:iam::$AWS_ACCOUNT_ID:policy/KarpenterControllerIAMPolicy"
        if ! aws iam get-policy --policy-arn $POLICY_ARN >/dev/null 2>&1; then
          cat > karpenter-policy.json <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Action": [
                "ssm:GetParameter",
                "iam:PassRole",
                "ec2:DescribeImages",
                "ec2:RunInstances",
                "ec2:DescribeSubnets",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeLaunchTemplates",
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceTypes",
                "ec2:DescribeInstanceTypeOfferings",
                "ec2:DescribeAvailabilityZones",
                "ec2:DeleteLaunchTemplate",
                "ec2:CreateTags",
                "ec2:CreateLaunchTemplate",
                "ec2:CreateFleet",
                "ec2:DescribeSpotPriceHistory",
                "pricing:GetProducts"
              ],
              "Resource": "*"
            },
            {
              "Effect": "Allow",
              "Action": "ec2:TerminateInstances",
              "Resource": "*",
              "Condition": {
                "StringLike": {
                  "ec2:ResourceTag/karpenter.sh/cluster": "*"
                }
              }
            }
          ]
        }
        EOF
          aws iam create-policy \
            --policy-name KarpenterControllerIAMPolicy \
            --policy-document file://karpenter-policy.json
        else
          echo "Karpenter policy already exists, skipping..."
        fi
    
    - name: Create Karpenter Controller IAM Role
      if: ${{ github.event.inputs.addon == 'karpenter' }}
      run: |
        OIDC_PROVIDER=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION \
          --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")
        cat > karpenter-trust.json <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Federated": "arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/${OIDC_PROVIDER}"
              },
              "Action": "sts:AssumeRoleWithWebIdentity",
              "Condition": {
                "StringEquals": {
                  "${OIDC_PROVIDER}:sub": "system:serviceaccount:karpenter:karpenter",
                  "${OIDC_PROVIDER}:aud": "sts.amazonaws.com"
                }
              }
            }
          ]
        }
        EOF
        if ! aws iam get-role --role-name KarpenterControllerIAMRole >/dev/null 2>&1; then
          aws iam create-role \
            --role-name KarpenterControllerIAMRole \
            --assume-role-policy-document file://karpenter-trust.json
          aws iam attach-role-policy \
            --role-name KarpenterControllerIAMRole \
            --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/KarpenterControllerIAMPolicy
        else
          echo "Karpenter controller role already exists, skipping..."
        fi
    
    - name: Create Karpenter Node IAM Role
      if: ${{ github.event.inputs.addon == 'karpenter' }}
      run: |
        cat > karpenter-node-trust.json <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Service": "ec2.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
        EOF
        if ! aws iam get-role --role-name KarpenterNodeInstanceProfile >/dev/null 2>&1; then
          aws iam create-role \
            --role-name KarpenterNodeInstanceProfile \
            --assume-role-policy-document file://karpenter-node-trust.json
          aws iam attach-role-policy \
            --role-name KarpenterNodeInstanceProfile \
            --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
          aws iam attach-role-policy \
            --role-name KarpenterNodeInstanceProfile \
            --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
          aws iam attach-role-policy \
            --role-name KarpenterNodeInstanceProfile \
            --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
          aws iam attach-role-policy \
            --role-name KarpenterNodeInstanceProfile \
            --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          aws iam create-instance-profile --instance-profile-name KarpenterNodeInstanceProfile
          aws iam add-role-to-instance-profile \
            --instance-profile-name KarpenterNodeInstanceProfile \
            --role-name KarpenterNodeInstanceProfile
        else
          echo "Karpenter node role already exists, skipping..."
        fi
    
    - name: Deploy Karpenter
      if: ${{ github.event.inputs.addon == 'karpenter' }}
      run: |
        helm repo add karpenter https://charts.karpenter.sh/
        helm repo update
        kubectl create namespace karpenter --dry-run=client -o yaml | kubectl apply -f -
        kubectl create serviceaccount karpenter -n karpenter --dry-run=client -o yaml | kubectl apply -f -
        kubectl annotate serviceaccount karpenter \
          -n karpenter \
          eks.amazonaws.com/role-arn=arn:aws:iam::$AWS_ACCOUNT_ID:role/KarpenterControllerIAMRole --overwrite
        helm upgrade --install karpenter karpenter/karpenter \
          --namespace karpenter \
          --set serviceAccount.create=false \
          --set serviceAccount.name=karpenter \
          --set clusterName=$CLUSTER_NAME \
          --set clusterEndpoint=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.endpoint" --output text) \
          --set aws.defaultInstanceProfile=KarpenterNodeInstanceProfile
